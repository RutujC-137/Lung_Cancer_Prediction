{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoVh1Ym-8Xrr",
        "outputId": "c783d02b-7acd-4342-f153-d98a4d69e9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- diagnosis_date: date (nullable = true)\n",
            " |-- cancer_stage: string (nullable = true)\n",
            " |-- family_history: string (nullable = true)\n",
            " |-- smoking_status: string (nullable = true)\n",
            " |-- bmi: double (nullable = true)\n",
            " |-- cholesterol_level: integer (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- asthma: integer (nullable = true)\n",
            " |-- cirrhosis: integer (nullable = true)\n",
            " |-- other_cancer: integer (nullable = true)\n",
            " |-- treatment_type: string (nullable = true)\n",
            " |-- end_treatment_date: date (nullable = true)\n",
            " |-- survived: integer (nullable = true)\n",
            "\n",
            "+---+----+------+-----------+--------------+------------+--------------+--------------+----+-----------------+------------+------+---------+------------+--------------+------------------+--------+\n",
            "| id| age|gender|    country|diagnosis_date|cancer_stage|family_history|smoking_status| bmi|cholesterol_level|hypertension|asthma|cirrhosis|other_cancer|treatment_type|end_treatment_date|survived|\n",
            "+---+----+------+-----------+--------------+------------+--------------+--------------+----+-----------------+------------+------+---------+------------+--------------+------------------+--------+\n",
            "|  1|64.0|  Male|     Sweden|    2016-04-05|     Stage I|           Yes|Passive Smoker|29.4|              199|           0|     0|        1|           0|  Chemotherapy|        2017-09-10|       0|\n",
            "|  2|50.0|Female|Netherlands|    2023-04-20|   Stage III|           Yes|Passive Smoker|41.2|              280|           1|     1|        0|           0|       Surgery|        2024-06-17|       1|\n",
            "|  3|65.0|Female|    Hungary|    2023-04-05|   Stage III|           Yes| Former Smoker|44.0|              268|           1|     1|        0|           0|      Combined|        2024-04-09|       0|\n",
            "|  4|51.0|Female|    Belgium|    2016-02-05|     Stage I|            No|Passive Smoker|43.0|              241|           1|     1|        0|           0|  Chemotherapy|        2017-04-23|       0|\n",
            "|  5|37.0|  Male| Luxembourg|    2023-11-29|     Stage I|            No|Passive Smoker|19.7|              178|           0|     0|        0|           0|      Combined|        2025-01-08|       0|\n",
            "+---+----+------+-----------+--------------+------------+--------------+--------------+----+-----------------+------------+------+---------+------------+--------------+------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"LungCancerPreprocessing\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/Lung_Cancer.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show schema and a few records\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVNoCc5F9AUU"
      },
      "outputs": [],
      "source": [
        "# Step 1: Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LungCancerPreprocessing\").getOrCreate()\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "df = spark.read.csv(\"/content/Lung_Cancer.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Basic Cleanup\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 4: String Indexing for Categorical Columns\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Identify categorical columns (excluding target)\n",
        "categorical_cols = [col for col, dtype in df.dtypes if dtype == 'string' and col != \"survived\"]\n",
        "\n",
        "# Create StringIndexers for each categorical column with '_indexed' suffix\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\") for col in categorical_cols]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Step 5: Select and Rename Proper Columns\n",
        "# Drop original string columns, keep numeric and indexed ones\n",
        "indexed_cols = [col+\"_indexed\" for col in categorical_cols]\n",
        "non_categorical_cols = [col for col in df.columns if col not in categorical_cols]\n",
        "\n",
        "# Combine everything for export\n",
        "final_cols = non_categorical_cols + indexed_cols\n",
        "\n",
        "# Select only those columns\n",
        "df_cleaned = df_indexed.select(*final_cols)\n",
        "\n",
        "# Step 6: Encode Target Column\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "if \"survived\" in df.columns:\n",
        "    label_indexer = StringIndexer(inputCol=\"survived\", outputCol=\"label\")\n",
        "    df_final = label_indexer.fit(df_cleaned).transform(df_cleaned)\n",
        "else:\n",
        "    raise Exception(\"‚ùå 'survived' column not found in the dataset!\")\n",
        "\n",
        "# Step 7: Undersampling to fix class imbalance\n",
        "label_counts = df_final.groupBy(\"label\").count().collect()\n",
        "min_count = min([row[\"count\"] for row in label_counts])\n",
        "\n",
        "balanced_df = None\n",
        "\n",
        "for row in label_counts:\n",
        "    cls_df = df_final.filter(df_final[\"label\"] == row[\"label\"]).limit(min_count)\n",
        "    balanced_df = cls_df if balanced_df is None else balanced_df.union(cls_df)\n",
        "\n",
        "# Step 8: Reorder Columns as per your request\n",
        "desired_order = [\n",
        "    \"id\", \"age\", \"bmi\", \"cholesterol_level\", \"hypertension\", \"asthma\", \"cirrhosis\", \"other_cancer\",\n",
        "    \"gender_indexed\", \"country_indexed\", \"cancer_stage_indexed\", \"family_history_indexed\",\n",
        "    \"smoking_status_indexed\", \"treatment_type_indexed\", \"survived\"\n",
        "]\n",
        "\n",
        "# Step 9: Export Final CSV with Required Format\n",
        "balanced_df.select(*desired_order) \\\n",
        "    .write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"lung_cancer_balanced_output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn9ug00hHzgh",
        "outputId": "a94571d2-97d6-4253-c9cd-8dd39367d3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "aw7t_60NJW_v",
        "outputId": "2b2384d7-c5fc-4a6d-887d-94b9b32e6158"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# üîπ Step 1: Load all CSV files generated by Spark\n",
        "csv_files = glob.glob(\"/content/lung_cancer_ml_ready_csv/*.csv\")\n",
        "\n",
        "# üîπ Step 2: Read and combine them into one DataFrame\n",
        "df_list = [pd.read_csv(f) for f in csv_files]\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------EDA----------------------#\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# üîé Basic Info\n",
        "print(\"üßæ Shape of dataset:\", df.shape)\n",
        "print(\"\\nüìã Column types:\\n\", df.dtypes)\n",
        "print(\"\\nüîç Missing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# üìà Distribution of target variable\n",
        "print(\"\\nüéØ Label Distribution:\")\n",
        "print(df['label'].value_counts(normalize=True))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title(\"Distribution of Target Label (Survived)\")\n",
        "plt.show()\n",
        "\n",
        "# üìä Boxplots to check outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "df.iloc[:, :10].boxplot()\n",
        "plt.title(\"üì¶ Boxplot of First 10 Features\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# üìâ Correlation Matrix\n",
        "plt.figure(figsize=(15, 10))\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=False)\n",
        "plt.title(\"üîó Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# üìä Histogram for a few features\n",
        "df.iloc[:, :6].hist(figsize=(12, 8), bins=20)\n",
        "plt.suptitle(\"üìä Feature Distributions\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# üîπ Step 3: Separate Features and Labels\n",
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "\n",
        "# üîπ Step 4: Split into Train & Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# üîπ Step 5: Train Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "print(\"üìä Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "print(\"‚úÖ Random Forest Accuracy:\", round(accuracy_score(y_test, rf_preds), 4))\n",
        "\n",
        "# üîπ Step 6: Train XGBoost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"\\nüìä XGBoost Classification Report:\\n\")\n",
        "print(classification_report(y_test, xgb_preds))\n",
        "print(\"‚úÖ XGBoost Accuracy:\", round(accuracy_score(y_test, xgb_preds), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlk7gvnvLh0G"
      },
      "source": [
        "üîç Pehle Performance samjho:\n",
        "‚úÖ Accuracy:\n",
        "Random Forest: 85.94%\n",
        "\n",
        "XGBoost: 85.86%\n",
        "\n",
        "üìà Classification Report ke hisaab se:\n",
        "Class 0 (No Cancer):\n",
        "\n",
        "Precision: 0.78 ‚Üí Prediction mein thoda galti ho raha hai.\n",
        "\n",
        "Recall: 1.00 ‚Üí Matlab jo actual non-cancer patients the, unko correctly pakad liya.\n",
        "\n",
        "Class 1 (Cancer):\n",
        "\n",
        "Precision: 1.00 ‚Üí Cancer bol diya to pakka cancer hai.\n",
        "\n",
        "Recall: 0.72 ‚Üí Matlab 28% actual cancer cases chhoot ja rahe hain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j960pLQv3OcD"
      },
      "outputs": [],
      "source": [
        "# üî∏ Save trained Random Forest model\n",
        "import pickle\n",
        "\n",
        "with open(\"basic_random_forest_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "# üî∏ Save trained XGBoost model\n",
        "with open(\"basic_xgboost_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_model, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOa1tEOQoPh6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# üîπ Step 1: Load all CSV files generated by Spark\n",
        "csv_files = glob.glob(\"/content/lung_cancer_ml_ready_csv/*.csv\")\n",
        "\n",
        "# üîπ Step 2: Read and combine them into one DataFrame\n",
        "df_list = [pd.read_csv(f) for f in csv_files]\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# üîπ Step 3: Separate Features and Labels\n",
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "\n",
        "# üîπ Step 4: Split into Train & Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------------------------\n",
        "# üîπ Step 5: Random Forest Hyperparameter Tuning\n",
        "# -----------------------------------------------\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "rf_random = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_grid_rf,\n",
        "    n_iter=10,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_random.fit(X_train, y_train)\n",
        "best_rf = rf_random.best_estimator_\n",
        "rf_preds = best_rf.predict(X_test)\n",
        "\n",
        "print(\"üìä Random Forest (Tuned) Classification Report:\\n\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "print(\"‚úÖ Random Forest Accuracy:\", round(accuracy_score(y_test, rf_preds), 4))\n",
        "\n",
        "# -----------------------------------------------\n",
        "# üîπ Step 6: XGBoost Hyperparameter Tuning\n",
        "# -----------------------------------------------\n",
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    param_distributions=param_grid_xgb,\n",
        "    n_iter=10,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_random.fit(X_train, y_train)\n",
        "best_xgb = xgb_random.best_estimator_\n",
        "xgb_preds = best_xgb.predict(X_test)\n",
        "\n",
        "print(\"\\nüìä XGBoost (Tuned) Classification Report:\\n\")\n",
        "print(classification_report(y_test, xgb_preds))\n",
        "print(\"‚úÖ XGBoost Accuracy:\", round(accuracy_score(y_test, xgb_preds), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yvfW1afguRe"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# ‚úÖ Save model using 'wb' and pickle.dump()\n",
        "#rand\n",
        "with open(\"best_random_forest_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_rf, f)\n",
        "\n",
        "with open(\"best_xgboost_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_xgb, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjIrqvT0ncrg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Load your cleaned dataset\n",
        "df = pd.read_csv(\"resampled_lung_cancer_data.csv\")  # CSV you got from PySpark\n",
        "\n",
        "# Step 2: Split features and target\n",
        "X = df.drop(\"Lung_Cancer\", axis=1)\n",
        "y = df[\"Lung_Cancer\"]\n",
        "\n",
        "# Step 3: OneHotEncoder for categorical columns\n",
        "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Save encoder as preprocessing pipeline\n",
        "joblib.dump(encoder, \"preprocessing_pipeline.pkl\")\n",
        "\n",
        "# Save feature names after encoding\n",
        "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "joblib.dump(encoded_feature_names.tolist(), \"feature_names.pkl\")\n",
        "\n",
        "print(\"‚úÖ preprocessing_pipeline.pkl and feature_names.pkl saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCmRPLOZ4tYj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
